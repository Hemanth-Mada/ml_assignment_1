{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd99b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e824b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MACHINE LEARNING ASSIGNMENT - RIDGE REGRESSION\n",
      "============================================================\n",
      "‚úÖ Files loaded successfully!\n",
      "Training data shape: (1200, 81)\n",
      "Test data shape: (260, 80)\n",
      "\n",
      "üìä Target variable: HotelValue\n",
      "Target statistics:\n",
      "count      1200.000000\n",
      "mean     181709.895833\n",
      "std       77638.660223\n",
      "min       34900.000000\n",
      "25%      130000.000000\n",
      "50%      165000.000000\n",
      "75%      215000.000000\n",
      "max      745000.000000\n",
      "Name: HotelValue, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Load the datasets ---\n",
    "print(\"=\"*60)\n",
    "print(\"MACHINE LEARNING ASSIGNMENT - RIDGE REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Reading from the Hotel-Property-Value-Dataset folder\n",
    "    train_df = pd.read_csv('../Hotel-Property-Value-Dataset/train.csv')\n",
    "    test_df = pd.read_csv('../Hotel-Property-Value-Dataset/test.csv')\n",
    "    sample_submission_df = pd.read_csv('../Hotel-Property-Value-Dataset/sample_submission.csv')\n",
    "    print(\"‚úÖ Files loaded successfully!\")\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Ensure the Hotel-Property-Value-Dataset folder contains train.csv, test.csv, and sample_submission.csv\")\n",
    "    exit()\n",
    "\n",
    "# --- Target Variable ---\n",
    "TARGET_VARIABLE = \"HotelValue\"\n",
    "\n",
    "if TARGET_VARIABLE not in train_df.columns:\n",
    "    print(f\"‚ùå Error: The target column '{TARGET_VARIABLE}' was not found in train.csv.\")\n",
    "    print(f\"Available columns are: {list(train_df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nüìä Target variable: {TARGET_VARIABLE}\")\n",
    "print(f\"Target statistics:\\n{train_df[TARGET_VARIABLE].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15c3947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: DATA PREPROCESSING\n",
      "============================================================\n",
      "Features in training data: 80\n",
      "Features in test data: 80\n",
      "\n",
      "üìà Numeric features (37): ['Id', 'PropertyClass', 'RoadAccessLength', 'LandArea', 'OverallQuality']...\n",
      "üìù Categorical features (43): ['ZoningCategory', 'RoadType', 'ServiceLaneType', 'PlotShape', 'LandElevation']...\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Data Preprocessing (Course Concepts) ---\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Separate features (X) from the target (y)\n",
    "X_train_full = train_df.drop([TARGET_VARIABLE], axis=1)\n",
    "y_train = train_df[TARGET_VARIABLE]\n",
    "X_test_full = test_df.copy()\n",
    "\n",
    "print(f\"Features in training data: {X_train_full.shape[1]}\")\n",
    "print(f\"Features in test data: {X_test_full.shape[1]}\")\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train_full.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nüìà Numeric features ({len(numeric_features)}): {numeric_features[:5]}...\" if len(numeric_features) > 5 else f\"\\nüìà Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"üìù Categorical features ({len(categorical_features)}): {categorical_features[:5]}...\" if len(categorical_features) > 5 else f\"üìù Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ace009ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned data loaded from ../Hotel-Property-Value-Dataset/train_cleaned_mada.csv\n",
      "   Shape: (1200, 81)\n",
      "‚úÖ Using cleaned data from CSV!\n",
      "Training features shape: (1200, 80)\n",
      "Target shape: (1200,)\n",
      "üîÑ Processing test data...\n",
      "\n",
      "============================================================\n",
      "STEP 3: FEATURE SCALING\n",
      "============================================================\n",
      "‚úÖ Features scaled successfully!\n",
      "Training features shape: (1200, 80)\n",
      "Test features shape: (260, 80)\n",
      "Feature means after scaling: [-9.47390314e-17  1.77635684e-17  2.54611147e-16 -7.69754630e-17\n",
      " -6.21724894e-17] (should be ~0)\n",
      "Feature stds after scaling: [1. 1. 1. 1. 1.] (should be ~1)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Handle Missing Values and Encode Categorical Variables ---\n",
    "\n",
    "# Configuration: Set your preferred cache directory here\n",
    "CACHE_DIR = '../Hotel-Property-Value-Dataset/'  # Change this path as needed\n",
    "\n",
    "def save_cleaned_data(X_train_processed, y_train, base_path=CACHE_DIR):\n",
    "    \"\"\"Save cleaned data as CSV file\"\"\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # Combine features and target into single dataframe\n",
    "    cleaned_df = X_train_processed.copy()\n",
    "    cleaned_df['HotelValue'] = y_train\n",
    "    \n",
    "    # Save as CSV\n",
    "    csv_path = os.path.join(base_path, 'train_cleaned_mada.csv')\n",
    "    cleaned_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Cleaned data saved to {csv_path}\")\n",
    "    print(f\"   Shape: {cleaned_df.shape}\")\n",
    "    return csv_path\n",
    "\n",
    "def load_cleaned_data(base_path=CACHE_DIR):\n",
    "    \"\"\"Load cleaned data if it exists\"\"\"\n",
    "    csv_path = os.path.join(base_path, 'train_cleaned_mada.csv')\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        cleaned_df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X_train_processed = cleaned_df.drop('HotelValue', axis=1)\n",
    "        y_train = cleaned_df['HotelValue']\n",
    "        \n",
    "        print(f\"‚úÖ Cleaned data loaded from {csv_path}\")\n",
    "        print(f\"   Shape: {cleaned_df.shape}\")\n",
    "        return X_train_processed, y_train, True\n",
    "    else:\n",
    "        print(\"‚ùå Cleaned data not found. Will perform preprocessing...\")\n",
    "        return None, None, False\n",
    "\n",
    "def preprocess_data(X_train, X_test, numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Preprocess the data by handling missing values and encoding categorical variables\n",
    "    Following course preprocessing concepts\n",
    "    \"\"\"\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_test_processed = X_test.copy()\n",
    "    \n",
    "    # Handle numeric features - Fill with median (robust to outliers)\n",
    "    for col in numeric_features:\n",
    "        if col in X_train_processed.columns:\n",
    "            median_val = X_train_processed[col].median()\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(median_val)\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(median_val)\n",
    "    \n",
    "    # Handle categorical features - Label encoding\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        if col in X_train_processed.columns:\n",
    "            # Fill missing values with mode (most frequent value)\n",
    "            mode_val = X_train_processed[col].mode()[0] if not X_train_processed[col].mode().empty else 'Unknown'\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(mode_val)\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(mode_val)\n",
    "            \n",
    "            # Label encode categorical variables\n",
    "            le = LabelEncoder()\n",
    "            # Fit on combined data to handle unseen categories in test set\n",
    "            combined_data = pd.concat([X_train_processed[col], X_test_processed[col]], axis=0)\n",
    "            le.fit(combined_data)\n",
    "            \n",
    "            X_train_processed[col] = le.transform(X_train_processed[col])\n",
    "            X_test_processed[col] = le.transform(X_test_processed[col])\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return X_train_processed, X_test_processed, label_encoders\n",
    "\n",
    "# Try to load cleaned data first\n",
    "X_train_processed, y_train_series, data_loaded = load_cleaned_data()\n",
    "\n",
    "if not data_loaded:\n",
    "    print(\"üîÑ Starting data preprocessing...\")\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_processed, X_test_processed, label_encoders = preprocess_data(\n",
    "        X_train_full, X_test_full, numeric_features, categorical_features\n",
    "    )\n",
    "\n",
    "    print(f\"\\nAfter preprocessing:\")\n",
    "    print(f\"‚úÖ Training data shape: {X_train_processed.shape}\")\n",
    "    print(f\"‚úÖ Test data shape: {X_test_processed.shape}\")\n",
    "    print(f\"‚úÖ Missing values in training data: {X_train_processed.isnull().sum().sum()}\")\n",
    "    print(f\"‚úÖ Missing values in test data: {X_test_processed.isnull().sum().sum()}\")\n",
    "\n",
    "    # Save cleaned training data as CSV\n",
    "    y_train_for_saving = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "    save_cleaned_data(X_train_processed, y_train_for_saving)\n",
    "    \n",
    "    # Convert y_train to pandas Series for consistency\n",
    "    y_train_series = pd.Series(y_train_for_saving, name='HotelValue')\n",
    "    \n",
    "else:\n",
    "    print(f\"‚úÖ Using cleaned data from CSV!\")\n",
    "    print(f\"Training features shape: {X_train_processed.shape}\")\n",
    "    print(f\"Target shape: {y_train_series.shape}\")\n",
    "    \n",
    "    # Still need to preprocess test data when loading from CSV\n",
    "    print(\"üîÑ Processing test data...\")\n",
    "    _, X_test_processed, label_encoders = preprocess_data(\n",
    "        X_train_full, X_test_full, numeric_features, categorical_features\n",
    "    )\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: FEATURE SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale all features using standardization (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_processed)\n",
    "X_test_scaled = scaler.transform(X_test_processed)\n",
    "\n",
    "print(f\"‚úÖ Features scaled successfully!\")\n",
    "print(f\"Training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test features shape: {X_test_scaled.shape}\")\n",
    "print(f\"Feature means after scaling: {np.mean(X_train_scaled, axis=0)[:5]} (should be ~0)\")\n",
    "print(f\"Feature stds after scaling: {np.std(X_train_scaled, axis=0)[:5]} (should be ~1)\")\n",
    "\n",
    "# Convert to numpy array for mathematical operations\n",
    "y_train = y_train_series.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99668638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: RIDGE REGRESSION - SCIKIT-LEARN IMPLEMENTATION\n",
      "============================================================\n",
      "üìê Using sklearn Ridge Regression:\n",
      "Objective: Minimize ||XŒ∏ - y||¬≤ + Œª||Œ∏||¬≤\n",
      "Training data shape: (1200, 80)\n",
      "\n",
      "üîç HYPERPARAMETER TUNING - TESTING Œ± VALUES:\n",
      "--------------------------------------------------\n",
      "‚úÖ Best Œ± (regularization) found via CV: 1000.0\n",
      "Œ± =     0.01 | R¬≤ = 0.8675 | Reg Penalty = 18,161,463 | ||Œ∏|| = 42616.27\n",
      "Œ± =     0.10 | R¬≤ = 0.8675 | Reg Penalty = 181,521,771 | ||Œ∏|| = 42605.37\n",
      "Œ± =     1.00 | R¬≤ = 0.8675 | Reg Penalty = 1,806,002,587 | ||Œ∏|| = 42497.09\n",
      "Œ± =    10.00 | R¬≤ = 0.8675 | Reg Penalty = 17,204,224,708 | ||Œ∏|| = 41477.98\n",
      "Œ± =   100.00 | R¬≤ = 0.8642 | Reg Penalty = 123,277,275,776 | ||Œ∏|| = 35110.86\n",
      "Œ± =  1000.00 | R¬≤ = 0.8307 | Reg Penalty = 554,133,705,024 | ||Œ∏|| = 23540.04\n",
      "\n",
      "‚úÖ OPTIMAL RIDGE REGRESSION MODEL:\n",
      "----------------------------------------\n",
      "Best Œ± (regularization): 1000.0\n",
      "Training R¬≤ score: 0.8307\n",
      "Number of features: 80\n",
      "Bias term (intercept): 181709.90\n",
      "First 5 feature weights: [ -522.55500954 -1947.88706652  -871.19963665   147.56884139\n",
      "  2809.29671588]\n",
      "Weight L2 norm: 23540.04\n",
      "\n",
      "üìä REGULARIZATION IMPACT ANALYSIS:\n",
      "----------------------------------------\n",
      "Least regularized (Œ±=0.01) weight norm: 42616.27\n",
      "Optimal Ridge weight norm: 23540.04\n",
      "Weight reduction: 44.8%\n"
     ]
    }
   ],
   "source": [
    "# === STEP 4: RIDGE REGRESSION USING SCIKIT-LEARN ===\n",
    "# Using sklearn's Ridge for efficiency and built-in cross-validation\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: RIDGE REGRESSION - SCIKIT-LEARN IMPLEMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"üìê Using sklearn Ridge Regression:\")\n",
    "print(\"Objective: Minimize ||XŒ∏ - y||¬≤ + Œª||Œ∏||¬≤\")\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "\n",
    "# Ridge regression hyperparameter (regularization strength)\n",
    "# Try different alpha values to find optimal regularization\n",
    "alpha_values = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "print(f\"\\nüîç HYPERPARAMETER TUNING - TESTING Œ± VALUES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Use RidgeCV for automatic cross-validation\n",
    "ridge_cv = RidgeCV(alphas=alpha_values, cv=5, scoring='r2')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_alpha = ridge_cv.alpha_\n",
    "print(f\"‚úÖ Best Œ± (regularization) found via CV: {best_alpha}\")\n",
    "\n",
    "# Train final model with best alpha\n",
    "model = Ridge(alpha=best_alpha)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test different alpha values for comparison\n",
    "all_results = {}\n",
    "for alpha in alpha_values:\n",
    "    ridge_temp = Ridge(alpha=alpha)\n",
    "    ridge_temp.fit(X_train_scaled, y_train)\n",
    "    r2_score = ridge_temp.score(X_train_scaled, y_train)\n",
    "    \n",
    "    # Calculate regularization penalty\n",
    "    reg_penalty = alpha * np.sum(ridge_temp.coef_ ** 2)\n",
    "    \n",
    "    all_results[alpha] = {\n",
    "        'model': ridge_temp,\n",
    "        'r2': r2_score,\n",
    "        'reg_penalty': reg_penalty,\n",
    "        'coef_norm': np.linalg.norm(ridge_temp.coef_)\n",
    "    }\n",
    "    \n",
    "    print(f\"Œ± = {alpha:8.2f} | R¬≤ = {r2_score:.4f} | Reg Penalty = {reg_penalty:,.0f} | ||Œ∏|| = {np.linalg.norm(ridge_temp.coef_):.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ OPTIMAL RIDGE REGRESSION MODEL:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Best Œ± (regularization): {best_alpha}\")\n",
    "print(f\"Training R¬≤ score: {model.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Bias term (intercept): {model.intercept_:.2f}\")\n",
    "print(f\"First 5 feature weights: {model.coef_[:5]}\")\n",
    "print(f\"Weight L2 norm: {np.linalg.norm(model.coef_):.2f}\")\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model.predict(X_train_scaled)\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Compare with least regularized version\n",
    "print(f\"\\nüìä REGULARIZATION IMPACT ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "unreg_results = all_results[0.01]  # Least regularized\n",
    "reg_results = all_results[best_alpha]\n",
    "\n",
    "print(f\"Least regularized (Œ±=0.01) weight norm: {unreg_results['coef_norm']:.2f}\")\n",
    "print(f\"Optimal Ridge weight norm: {reg_results['coef_norm']:.2f}\")\n",
    "print(f\"Weight reduction: {(1 - reg_results['coef_norm']/unreg_results['coef_norm'])*100:.1f}%\")\n",
    "\n",
    "# Store coefficients for analysis (including intercept)\n",
    "weights = np.concatenate([[model.intercept_], model.coef_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d3432bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: ERROR FUNCTION ANALYSIS\n",
      "============================================================\n",
      "üìä TRAINING SET ERROR ANALYSIS:\n",
      "----------------------------------------\n",
      "MSE         : 1,019,573,087\n",
      "RMSE        : $31,930.75\n",
      "MAE         : $19,068.46\n",
      "R¬≤          : 0.8307\n",
      "Adjusted R¬≤ : 0.8186\n",
      "MAPE        : 10.63%\n",
      "\n",
      "üìà PREDICTION QUALITY ANALYSIS:\n",
      "----------------------------------------\n",
      "Mean residual: $-0.00\n",
      "Std of residuals: $31,930.75\n",
      "Min prediction: $46,436.98\n",
      "Max prediction: $494,678.16\n",
      "Predictions in range [0, max_actual]: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# === STEP 5: ERROR FUNCTION ANALYSIS ===\n",
    "# Following course concepts: Multiple Error Functions for Model Evaluation\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: ERROR FUNCTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_error_functions(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate multiple error functions as taught in course\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    \n",
    "    # 1. Mean Squared Error (MSE) - L2 Loss\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # 2. Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # 3. Mean Absolute Error (MAE) - L1 Loss\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # 4. R-squared (Coefficient of Determination)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)  # Residual sum of squares\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    # 5. Adjusted R-squared\n",
    "    n_features = X_train_scaled.shape[1]\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - n_features - 1)\n",
    "    \n",
    "    # 6. Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'Adjusted R¬≤': adj_r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "# Calculate error functions for training data\n",
    "train_errors = calculate_error_functions(y_train, train_predictions)\n",
    "\n",
    "print(\"üìä TRAINING SET ERROR ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "for metric, value in train_errors.items():\n",
    "    if metric in ['MAPE']:\n",
    "        print(f\"{metric:12}: {value:.2f}%\")\n",
    "    elif metric in ['MSE']:\n",
    "        print(f\"{metric:12}: {value:,.0f}\")\n",
    "    elif metric in ['RMSE', 'MAE']:\n",
    "        print(f\"{metric:12}: ${value:,.2f}\")\n",
    "    else:\n",
    "        print(f\"{metric:12}: {value:.4f}\")\n",
    "\n",
    "# Analysis of prediction quality\n",
    "print(f\"\\nüìà PREDICTION QUALITY ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "residuals = y_train - train_predictions\n",
    "print(f\"Mean residual: ${np.mean(residuals):,.2f}\")\n",
    "print(f\"Std of residuals: ${np.std(residuals):,.2f}\")\n",
    "print(f\"Min prediction: ${np.min(train_predictions):,.2f}\")\n",
    "print(f\"Max prediction: ${np.max(train_predictions):,.2f}\")\n",
    "print(f\"Predictions in range [0, max_actual]: {np.sum((train_predictions >= 0) & (train_predictions <= np.max(y_train))) / len(train_predictions) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52e07b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 6: MODEL DIAGNOSTICS AND VALIDATION\n",
      "============================================================\n",
      "üîç PREDICTION EXAMPLES (First 10 samples):\n",
      "--------------------------------------------------\n",
      "     Actual  Predicted   Residual  Abs_Error\n",
      "0  395000.0  284021.51  110978.49  110978.49\n",
      "1  165000.0  188176.18  -23176.18   23176.18\n",
      "2  128200.0  122934.61    5265.39    5265.39\n",
      "3  275000.0  242525.45   32474.55   32474.55\n",
      "4  311872.0  326352.84  -14480.84   14480.84\n",
      "5  214000.0  240261.34  -26261.34   26261.34\n",
      "6  153500.0  190146.54  -36646.54   36646.54\n",
      "7  144000.0  158461.47  -14461.47   14461.47\n",
      "8  115000.0  125563.42  -10563.42   10563.42\n",
      "9  180000.0  180532.85    -532.85     532.85\n",
      "\n",
      "üìä MODEL COMPLEXITY ANALYSIS:\n",
      "----------------------------------------\n",
      "Number of training samples: 1200\n",
      "Number of features: 80\n",
      "Parameters to data ratio: 81/1200 = 0.0675\n",
      "\n",
      "‚öñÔ∏è LEARNED PARAMETERS ANALYSIS:\n",
      "----------------------------------------\n",
      "Intercept (bias): 181709.8958\n",
      "Largest positive weight: 8785.4688\n",
      "Largest negative weight: -6014.6779\n",
      "Weight standard deviation: 2529.2406\n",
      "\n",
      "üìâ RESIDUAL ANALYSIS:\n",
      "----------------------------------------\n",
      "Residual mean (should be ~0): -0.0000\n",
      "Residual std: 31930.75\n",
      "Residual skewness: 0.6616\n",
      "\n",
      "‚úÖ MODEL VALIDATION COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: MODEL DIAGNOSTICS AND VALIDATION ===\n",
    "# Following course concepts: Model Validation and Diagnostics\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: MODEL DIAGNOSTICS AND VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Prediction Examples (Sample Analysis)\n",
    "print(\"üîç PREDICTION EXAMPLES (First 10 samples):\")\n",
    "print(\"-\" * 50)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_train[:10],\n",
    "    'Predicted': train_predictions[:10],\n",
    "    'Residual': y_train[:10] - train_predictions[:10],\n",
    "    'Abs_Error': np.abs(y_train[:10] - train_predictions[:10])\n",
    "})\n",
    "print(comparison_df.round(2))\n",
    "\n",
    "# 2. Model Complexity Analysis\n",
    "print(f\"\\nüìä MODEL COMPLEXITY ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of training samples: {len(y_train)}\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Parameters to data ratio: {len(weights)}/{len(y_train)} = {len(weights)/len(y_train):.4f}\")\n",
    "\n",
    "# 3. Weight Analysis\n",
    "print(f\"\\n‚öñÔ∏è LEARNED PARAMETERS ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Intercept (bias): {weights[0]:.4f}\")\n",
    "print(f\"Largest positive weight: {np.max(weights[1:]):.4f}\")\n",
    "print(f\"Largest negative weight: {np.min(weights[1:]):.4f}\")\n",
    "print(f\"Weight standard deviation: {np.std(weights[1:]):.4f}\")\n",
    "\n",
    "# 4. Residual Analysis\n",
    "print(f\"\\nüìâ RESIDUAL ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "residuals = y_train - train_predictions\n",
    "print(f\"Residual mean (should be ~0): {np.mean(residuals):.4f}\")\n",
    "print(f\"Residual std: {np.std(residuals):.2f}\")\n",
    "print(f\"Residual skewness: {np.mean(((residuals - np.mean(residuals)) / np.std(residuals)) ** 3):.4f}\")\n",
    "\n",
    "# 5. Prediction Bounds Analysis\n",
    "negative_predictions = np.sum(train_predictions < 0)\n",
    "if negative_predictions > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {negative_predictions} negative predictions detected!\")\n",
    "    print(\"This suggests the model may need constraints or regularization.\")\n",
    "\n",
    "print(f\"\\n‚úÖ MODEL VALIDATION COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7c11c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 7: RIDGE MODEL ANALYSIS\n",
      "============================================================\n",
      "üìê RIDGE REGRESSION MODEL PROPERTIES:\n",
      "--------------------------------------------------\n",
      "Optimal Œ± (regularization): 1000.0\n",
      "Intercept: 181709.8958\n",
      "Number of coefficients: 80\n",
      "Coefficient L2 norm: 23540.0447\n",
      "Largest positive coefficient: 8785.4688\n",
      "Largest negative coefficient: -6014.6779\n",
      "Coefficient standard deviation: 2529.2406\n",
      "\n",
      "üìä MODEL PERFORMANCE:\n",
      "--------------------------------------------------\n",
      "Training R¬≤ score: 0.8307\n",
      "Training R¬≤ percentage: 83.07%\n",
      "Cross-validation R¬≤ mean: 0.7940 ¬± 0.0549\n",
      "\n",
      "üéØ REGULARIZATION EFFECT ANALYSIS:\n",
      "--------------------------------------------------\n",
      "Œ± Value | R¬≤ Score | Coef Norm | Reg Penalty\n",
      "---------------------------------------------\n",
      "   0.01 |   0.8675 |  42616.27 |  18,161,463\n",
      "   0.10 |   0.8675 |  42605.37 | 181,521,771\n",
      "   1.00 |   0.8675 |  42497.09 | 1,806,002,587\n",
      "  10.00 |   0.8675 |  41477.98 | 17,204,224,708\n",
      " 100.00 |   0.8642 |  35110.86 | 123,277,275,776\n",
      "1000.00 |   0.8307 |  23540.04 | 554,133,705,024\n",
      "\n",
      "üèÜ TOP 10 MOST IMPORTANT FEATURES:\n",
      "--------------------------------------------------\n",
      "        Feature  Coefficient  Abs_Coefficient\n",
      " OverallQuality  8785.468839      8785.468839\n",
      "     UsableArea  7313.780200      7313.780200\n",
      " KitchenQuality -6014.677937      6014.677937\n",
      " BasementHeight -5794.757426      5794.757426\n",
      "ExteriorQuality -5415.907464      5415.907464\n",
      "GroundFloorArea  5081.967579      5081.967579\n",
      "     TotalRooms  4882.697273      4882.697273\n",
      "    PoolQuality -4747.936884      4747.936884\n",
      "ParkingCapacity  4657.279791      4657.279791\n",
      "        Lounges  4359.121449      4359.121449\n",
      "\n",
      "üî¢ MODEL COMPLEXITY:\n",
      "--------------------------------------------------\n",
      "Number of samples: 1200\n",
      "Number of parameters: 81\n",
      "Degrees of freedom (residual): 1119\n",
      "Parameter/Sample ratio: 0.068\n",
      "\n",
      "‚öñÔ∏è REGULARIZATION ASSESSMENT:\n",
      "--------------------------------------------------\n",
      "Regularization penalty: 554,133,705,024\n",
      "Data fit term (MSE): 1,019,573,087\n",
      "Total objective value: 555,153,278,111\n",
      "üîí Strong regularization - emphasizing generalization\n"
     ]
    }
   ],
   "source": [
    "# === STEP 7: MODEL ANALYSIS ===\n",
    "# Analysis of the sklearn Ridge Regression model\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: RIDGE MODEL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Ridge Regression Model Properties\n",
    "print(\"üìê RIDGE REGRESSION MODEL PROPERTIES:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Optimal Œ± (regularization): {model.alpha}\")\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "print(f\"Number of coefficients: {len(model.coef_)}\")\n",
    "print(f\"Coefficient L2 norm: {np.linalg.norm(model.coef_):.4f}\")\n",
    "print(f\"Largest positive coefficient: {np.max(model.coef_):.4f}\")\n",
    "print(f\"Largest negative coefficient: {np.min(model.coef_):.4f}\")\n",
    "print(f\"Coefficient standard deviation: {np.std(model.coef_):.4f}\")\n",
    "\n",
    "# 2. Model Performance Metrics\n",
    "print(f\"\\nüìä MODEL PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "train_r2 = model.score(X_train_scaled, y_train)\n",
    "print(f\"Training R¬≤ score: {train_r2:.4f}\")\n",
    "print(f\"Training R¬≤ percentage: {train_r2*100:.2f}%\")\n",
    "\n",
    "# Cross-validation score\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "print(f\"Cross-validation R¬≤ mean: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 3. Regularization Effect Analysis\n",
    "print(f\"\\nüéØ REGULARIZATION EFFECT ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Œ± Value | R¬≤ Score | Coef Norm | Reg Penalty\")\n",
    "print(\"-\" * 45)\n",
    "for alpha in sorted(all_results.keys()):\n",
    "    result = all_results[alpha]\n",
    "    print(f\"{alpha:7.2f} | {result['r2']:8.4f} | {result['coef_norm']:9.2f} | {result['reg_penalty']:11,.0f}\")\n",
    "\n",
    "# 4. Feature Importance (absolute coefficient values)\n",
    "print(f\"\\nüèÜ TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "if hasattr(X_train_processed, 'columns'):\n",
    "    feature_names = X_train_processed.columns\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': model.coef_,\n",
    "        'Abs_Coefficient': np.abs(model.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "else:\n",
    "    # If we don't have feature names, show indices\n",
    "    coef_indices = np.argsort(np.abs(model.coef_))[::-1][:10]\n",
    "    for i, idx in enumerate(coef_indices):\n",
    "        print(f\"Feature {idx:2d}: {model.coef_[idx]:8.4f}\")\n",
    "\n",
    "# 5. Model Complexity Analysis\n",
    "print(f\"\\nüî¢ MODEL COMPLEXITY:\")\n",
    "print(\"-\" * 50)\n",
    "n_samples = len(y_train)\n",
    "n_params = len(model.coef_) + 1  # +1 for intercept\n",
    "df_residual = n_samples - n_params\n",
    "\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Number of parameters: {n_params}\")\n",
    "print(f\"Degrees of freedom (residual): {df_residual}\")\n",
    "print(f\"Parameter/Sample ratio: {n_params/n_samples:.3f}\")\n",
    "\n",
    "# 6. Regularization Strength Assessment\n",
    "print(f\"\\n‚öñÔ∏è REGULARIZATION ASSESSMENT:\")\n",
    "print(\"-\" * 50)\n",
    "reg_penalty = model.alpha * np.sum(model.coef_ ** 2)\n",
    "print(f\"Regularization penalty: {reg_penalty:,.0f}\")\n",
    "print(f\"Data fit term (MSE): {np.mean((y_train - train_predictions) ** 2):,.0f}\")\n",
    "print(f\"Total objective value: {np.mean((y_train - train_predictions) ** 2) + reg_penalty:,.0f}\")\n",
    "\n",
    "if model.alpha < 1.0:\n",
    "    print(\"‚úÖ Light regularization - preserving model flexibility\")\n",
    "elif model.alpha < 100.0:\n",
    "    print(\"‚öñÔ∏è Moderate regularization - good bias-variance balance\")\n",
    "else:\n",
    "    print(\"üîí Strong regularization - emphasizing generalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efef159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 8: KAGGLE SUBMISSION PREPARATION\n",
      "============================================================\n",
      "üìÅ LOADING TEST DATA AND MAKING PREDICTIONS:\n",
      "--------------------------------------------------\n",
      "Test data shape: (260, 80)\n",
      "‚úÖ Test predictions generated for 260 samples\n",
      "Prediction range: [52475.72, 460542.59]\n",
      "üì§ SUBMISSION FILE CREATED:\n",
      "--------------------------------------------------\n",
      "File saved at: /Users/hemanthmada/vscodeProjects/ml_assignment_1/submissions/2_ridge_regression_sklearn.csv\n",
      "Submission shape: (260, 2)\n",
      "\n",
      "First 5 predictions:\n",
      "     Id     HotelValue\n",
      "0   893  144645.784628\n",
      "1  1106  302298.314606\n",
      "2   414  111196.279900\n",
      "3   523  163626.201948\n",
      "4  1037  306632.375271\n",
      "\n",
      "============================================================\n",
      "üéì SKLEARN RIDGE REGRESSION SUMMARY\n",
      "============================================================\n",
      "‚úÖ Step 1: Data Loading and Exploration - COMPLETED\n",
      "‚úÖ Step 2: Data Preprocessing and Feature Engineering - COMPLETED\n",
      "‚úÖ Step 3: Feature Scaling - COMPLETED\n",
      "‚úÖ Step 4: Sklearn Ridge Regression with CV - COMPLETED\n",
      "‚úÖ Step 5: Comprehensive Error Function Analysis - COMPLETED\n",
      "‚úÖ Step 6: Model Diagnostics and Validation - COMPLETED\n",
      "‚úÖ Step 7: Ridge Model Analysis - COMPLETED\n",
      "‚úÖ Step 8: Kaggle Submission Preparation - COMPLETED\n",
      "\n",
      "üèÜ SKLEARN RIDGE REGRESSION MODEL READY!\n",
      "üéØ Optimal Œ±: 1000.0 | Training R¬≤: 0.8307\n",
      "üìä Cross-validation R¬≤: 0.7940 ¬± 0.0549\n",
      "üéØ Optimal Œ±: 1000.0 | Training R¬≤: 0.8307\n",
      "üìä Cross-validation R¬≤: 0.7940 ¬± 0.0549\n"
     ]
    }
   ],
   "source": [
    "# === STEP 8: KAGGLE SUBMISSION PREPARATION ===\n",
    "# Final step: Prepare submission file for Kaggle competition\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8: KAGGLE SUBMISSION PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load test data and make predictions\n",
    "print(\"üìÅ LOADING TEST DATA AND MAKING PREDICTIONS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test data was already preprocessed and scaled in Step 3\n",
    "print(f\"Test data shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# Make predictions using our trained sklearn Ridge model\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"‚úÖ Test predictions generated for {len(test_predictions)} samples\")\n",
    "print(f\"Prediction range: [{test_predictions.min():.2f}, {test_predictions.max():.2f}]\")\n",
    "\n",
    "# Create submission file using actual test IDs\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_df['Id'].values,  # Use actual IDs from test dataset\n",
    "    'HotelValue': test_predictions  # Use HotelValue as per sample submission format\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = '/Users/hemanthmada/vscodeProjects/ml_assignment_1/submissions/2_ridge_regression_sklearn.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"üì§ SUBMISSION FILE CREATED:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"File saved at: {submission_path}\")\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Final summary of the entire Ridge regression implementation\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üéì SKLEARN RIDGE REGRESSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Step 1: Data Loading and Exploration - COMPLETED\")\n",
    "print(\"‚úÖ Step 2: Data Preprocessing and Feature Engineering - COMPLETED\")\n",
    "print(\"‚úÖ Step 3: Feature Scaling - COMPLETED\")\n",
    "print(\"‚úÖ Step 4: Sklearn Ridge Regression with CV - COMPLETED\")\n",
    "print(\"‚úÖ Step 5: Comprehensive Error Function Analysis - COMPLETED\")\n",
    "print(\"‚úÖ Step 6: Model Diagnostics and Validation - COMPLETED\")\n",
    "print(\"‚úÖ Step 7: Ridge Model Analysis - COMPLETED\")\n",
    "print(\"‚úÖ Step 8: Kaggle Submission Preparation - COMPLETED\")\n",
    "print(f\"\\nüèÜ SKLEARN RIDGE REGRESSION MODEL READY!\")\n",
    "print(f\"üéØ Optimal Œ±: {model.alpha} | Training R¬≤: {model.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"üìä Cross-validation R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
