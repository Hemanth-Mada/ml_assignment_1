{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd99b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e824b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MACHINE LEARNING ASSIGNMENT - LASSO REGRESSION\n",
      "============================================================\n",
      "‚úÖ Files loaded successfully!\n",
      "Training data shape: (1200, 81)\n",
      "Test data shape: (260, 80)\n",
      "\n",
      "üìä Target variable: HotelValue\n",
      "Target statistics:\n",
      "count      1200.000000\n",
      "mean     181709.895833\n",
      "std       77638.660223\n",
      "min       34900.000000\n",
      "25%      130000.000000\n",
      "50%      165000.000000\n",
      "75%      215000.000000\n",
      "max      745000.000000\n",
      "Name: HotelValue, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Load the datasets ---\n",
    "print(\"=\"*60)\n",
    "print(\"MACHINE LEARNING ASSIGNMENT - LASSO REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Reading from the Hotel-Property-Value-Dataset folder\n",
    "    train_df = pd.read_csv('../Hotel-Property-Value-Dataset/train.csv')\n",
    "    test_df = pd.read_csv('../Hotel-Property-Value-Dataset/test.csv')\n",
    "    sample_submission_df = pd.read_csv('../Hotel-Property-Value-Dataset/sample_submission.csv')\n",
    "    print(\"‚úÖ Files loaded successfully!\")\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Ensure the Hotel-Property-Value-Dataset folder contains train.csv, test.csv, and sample_submission.csv\")\n",
    "    exit()\n",
    "\n",
    "# --- Target Variable ---\n",
    "TARGET_VARIABLE = \"HotelValue\"\n",
    "\n",
    "if TARGET_VARIABLE not in train_df.columns:\n",
    "    print(f\"‚ùå Error: The target column '{TARGET_VARIABLE}' was not found in train.csv.\")\n",
    "    print(f\"Available columns are: {list(train_df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nüìä Target variable: {TARGET_VARIABLE}\")\n",
    "print(f\"Target statistics:\\n{train_df[TARGET_VARIABLE].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c3947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: DATA PREPROCESSING\n",
      "============================================================\n",
      "Features in training data: 80\n",
      "Features in test data: 80\n",
      "\n",
      "üìà Numeric features (37): ['Id', 'PropertyClass', 'RoadAccessLength', 'LandArea', 'OverallQuality']...\n",
      "üìù Categorical features (43): ['ZoningCategory', 'RoadType', 'ServiceLaneType', 'PlotShape', 'LandElevation']...\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Data Preprocessing (Course Concepts) ---\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Separate features (X) from the target (y)\n",
    "X_train_full = train_df.drop([TARGET_VARIABLE], axis=1)\n",
    "y_train = train_df[TARGET_VARIABLE]\n",
    "X_test_full = test_df.copy()\n",
    "\n",
    "print(f\"Features in training data: {X_train_full.shape[1]}\")\n",
    "print(f\"Features in test data: {X_test_full.shape[1]}\")\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train_full.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nüìà Numeric features ({len(numeric_features)}): {numeric_features[:5]}...\" if len(numeric_features) > 5 else f\"\\nüìà Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"üìù Categorical features ({len(categorical_features)}): {categorical_features[:5]}...\" if len(categorical_features) > 5 else f\"üìù Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace009ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned data loaded from ../Hotel-Property-Value-Dataset/train_cleaned_mada.csv\n",
      "   Shape: (1200, 81)\n",
      "‚úÖ Using cleaned data from CSV!\n",
      "Training features shape: (1200, 80)\n",
      "Target shape: (1200,)\n",
      "üîÑ Processing test data...\n",
      "\n",
      "============================================================\n",
      "STEP 3: FEATURE SCALING\n",
      "============================================================\n",
      "‚úÖ Features scaled successfully!\n",
      "Training features shape: (1200, 80)\n",
      "Test features shape: (260, 80)\n",
      "Feature means after scaling: [-9.47390314e-17  1.77635684e-17  2.54611147e-16 -7.69754630e-17\n",
      " -6.21724894e-17] (should be ~0)\n",
      "Feature stds after scaling: [1. 1. 1. 1. 1.] (should be ~1)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Handle Missing Values and Encode Categorical Variables ---\n",
    "\n",
    "# Configuration: Set your preferred cache directory here\n",
    "CACHE_DIR = '../Hotel-Property-Value-Dataset/'  # Change this path as needed\n",
    "\n",
    "def save_cleaned_data(X_train_processed, y_train, base_path=CACHE_DIR):\n",
    "    \"\"\"Save cleaned data as CSV file\"\"\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # Combine features and target into single dataframe\n",
    "    cleaned_df = X_train_processed.copy()\n",
    "    cleaned_df['HotelValue'] = y_train\n",
    "    \n",
    "    # Save as CSV\n",
    "    csv_path = os.path.join(base_path, 'train_cleaned_mada.csv')\n",
    "    cleaned_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Cleaned data saved to {csv_path}\")\n",
    "    print(f\"   Shape: {cleaned_df.shape}\")\n",
    "    return csv_path\n",
    "\n",
    "def load_cleaned_data(base_path=CACHE_DIR):\n",
    "    \"\"\"Load cleaned data if it exists\"\"\"\n",
    "    csv_path = os.path.join(base_path, 'train_cleaned_mada.csv')\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        cleaned_df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X_train_processed = cleaned_df.drop('HotelValue', axis=1)\n",
    "        y_train = cleaned_df['HotelValue']\n",
    "        \n",
    "        print(f\"‚úÖ Cleaned data loaded from {csv_path}\")\n",
    "        print(f\"   Shape: {cleaned_df.shape}\")\n",
    "        return X_train_processed, y_train, True\n",
    "    else:\n",
    "        print(\"‚ùå Cleaned data not found. Will perform preprocessing...\")\n",
    "        return None, None, False\n",
    "\n",
    "def preprocess_data(X_train, X_test, numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Preprocess the data by handling missing values and encoding categorical variables\n",
    "    Following course preprocessing concepts\n",
    "    \"\"\"\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_test_processed = X_test.copy()\n",
    "    \n",
    "    # Handle numeric features - Fill with median (robust to outliers)\n",
    "    for col in numeric_features:\n",
    "        if col in X_train_processed.columns:\n",
    "            median_val = X_train_processed[col].median()\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(median_val)\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(median_val)\n",
    "    \n",
    "    # Handle categorical features - Label encoding\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        if col in X_train_processed.columns:\n",
    "            # Fill missing values with mode (most frequent value)\n",
    "            mode_val = X_train_processed[col].mode()[0] if not X_train_processed[col].mode().empty else 'Unknown'\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(mode_val)\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(mode_val)\n",
    "            \n",
    "            # Label encode categorical variables\n",
    "            le = LabelEncoder()\n",
    "            # Fit on combined data to handle unseen categories in test set\n",
    "            combined_data = pd.concat([X_train_processed[col], X_test_processed[col]], axis=0)\n",
    "            le.fit(combined_data)\n",
    "            \n",
    "            X_train_processed[col] = le.transform(X_train_processed[col])\n",
    "            X_test_processed[col] = le.transform(X_test_processed[col])\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return X_train_processed, X_test_processed, label_encoders\n",
    "\n",
    "# Try to load cleaned data first\n",
    "X_train_processed, y_train_series, data_loaded = load_cleaned_data()\n",
    "\n",
    "if not data_loaded:\n",
    "    print(\"üîÑ Starting data preprocessing...\")\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_processed, X_test_processed, label_encoders = preprocess_data(\n",
    "        X_train_full, X_test_full, numeric_features, categorical_features\n",
    "    )\n",
    "\n",
    "    print(f\"\\nAfter preprocessing:\")\n",
    "    print(f\"‚úÖ Training data shape: {X_train_processed.shape}\")\n",
    "    print(f\"‚úÖ Test data shape: {X_test_processed.shape}\")\n",
    "    print(f\"‚úÖ Missing values in training data: {X_train_processed.isnull().sum().sum()}\")\n",
    "    print(f\"‚úÖ Missing values in test data: {X_test_processed.isnull().sum().sum()}\")\n",
    "\n",
    "    # Save cleaned training data as CSV\n",
    "    y_train_for_saving = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "    save_cleaned_data(X_train_processed, y_train_for_saving)\n",
    "    \n",
    "    # Convert y_train to pandas Series for consistency\n",
    "    y_train_series = pd.Series(y_train_for_saving, name='HotelValue')\n",
    "    \n",
    "else:\n",
    "    print(f\"‚úÖ Using cleaned data from CSV!\")\n",
    "    print(f\"Training features shape: {X_train_processed.shape}\")\n",
    "    print(f\"Target shape: {y_train_series.shape}\")\n",
    "    \n",
    "    # Still need to preprocess test data when loading from CSV\n",
    "    print(\"üîÑ Processing test data...\")\n",
    "    _, X_test_processed, label_encoders = preprocess_data(\n",
    "        X_train_full, X_test_full, numeric_features, categorical_features\n",
    "    )\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: FEATURE SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale all features using standardization (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_processed)\n",
    "X_test_scaled = scaler.transform(X_test_processed)\n",
    "\n",
    "print(f\"‚úÖ Features scaled successfully!\")\n",
    "print(f\"Training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test features shape: {X_test_scaled.shape}\")\n",
    "print(f\"Feature means after scaling: {np.mean(X_train_scaled, axis=0)[:5]} (should be ~0)\")\n",
    "print(f\"Feature stds after scaling: {np.std(X_train_scaled, axis=0)[:5]} (should be ~1)\")\n",
    "\n",
    "# Convert to numpy array for mathematical operations\n",
    "y_train = y_train_series.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99668638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: LASSO REGRESSION - SCIKIT-LEARN IMPLEMENTATION\n",
      "============================================================\n",
      "üìê Using sklearn LASSO Regression:\n",
      "Objective: Minimize ||XŒ∏ - y||¬≤ + Œª||Œ∏||‚ÇÅ\n",
      "Key Feature: Automatic feature selection via L1 regularization\n",
      "Training data shape: (1200, 80)\n",
      "\n",
      "üîç HYPERPARAMETER TUNING - TESTING Œ± VALUES:\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Best Œ± (regularization) found via CV: 5000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hemanthmada/vscodeProjects/ml_assignment_1/.venv/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.585e+09, tolerance: 7.227e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Œ± =      0.0 | R¬≤ = 0.8675 | Active = 80/80 | Sparsity =   0.0% | L1 = 242187.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hemanthmada/vscodeProjects/ml_assignment_1/.venv/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.588e+09, tolerance: 7.227e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Œ± =      0.1 | R¬≤ = 0.8675 | Active = 80/80 | Sparsity =   0.0% | L1 = 241915.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hemanthmada/vscodeProjects/ml_assignment_1/.venv/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.612e+09, tolerance: 7.227e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Œ± =      1.0 | R¬≤ = 0.8675 | Active = 79/80 | Sparsity =   1.2% | L1 = 240441.15\n",
      "Œ± =     10.0 | R¬≤ = 0.8675 | Active = 78/80 | Sparsity =   2.5% | L1 = 235433.79\n",
      "Œ± =    100.0 | R¬≤ = 0.8673 | Active = 74/80 | Sparsity =   7.5% | L1 = 219356.68\n",
      "Œ± =   1000.0 | R¬≤ = 0.8578 | Active = 48/80 | Sparsity =  40.0% | L1 = 158683.84\n",
      "Œ± =   5000.0 | R¬≤ = 0.8072 | Active = 21/80 | Sparsity =  73.8% | L1 = 93342.31\n",
      "\n",
      "‚úÖ OPTIMAL LASSO REGRESSION MODEL:\n",
      "----------------------------------------\n",
      "Best Œ± (regularization): 5000.0\n",
      "Training R¬≤ score: 0.8072\n",
      "Number of features: 80\n",
      "Active features: 21/80\n",
      "Sparsity: 73.8% of features set to zero\n",
      "Bias term (intercept): 181709.90\n",
      "Non-zero coefficients (first 5): [-2784.31349629  2085.63731527 22065.37810362  2353.15126429\n",
      "   911.78349076]\n",
      "Coefficient L1 norm: 93342.31\n",
      "\n",
      "üìä FEATURE SELECTION ANALYSIS:\n",
      "----------------------------------------\n",
      "Features selected by LASSO: 21\n",
      "Features eliminated by LASSO: 59\n",
      "Feature selection ratio: 26.2%\n",
      "\n",
      "üìà REGULARIZATION PATH ANALYSIS:\n",
      "--------------------------------------------------\n",
      "Œ±        | R¬≤      | Active | Sparsity | L1 Norm\n",
      "--------------------------------------------------\n",
      "     0.0 | 0.8675 |   80   |    0.0% | 242187.98\n",
      "     0.1 | 0.8675 |   80   |    0.0% | 241915.78\n",
      "     1.0 | 0.8675 |   79   |    1.2% | 240441.15\n",
      "    10.0 | 0.8675 |   78   |    2.5% | 235433.79\n",
      "   100.0 | 0.8673 |   74   |    7.5% | 219356.68\n",
      "  1000.0 | 0.8578 |   48   |   40.0% | 158683.84\n",
      "  5000.0 | 0.8072 |   21   |   73.8% | 93342.31\n",
      "Œ± =     10.0 | R¬≤ = 0.8675 | Active = 78/80 | Sparsity =   2.5% | L1 = 235433.79\n",
      "Œ± =    100.0 | R¬≤ = 0.8673 | Active = 74/80 | Sparsity =   7.5% | L1 = 219356.68\n",
      "Œ± =   1000.0 | R¬≤ = 0.8578 | Active = 48/80 | Sparsity =  40.0% | L1 = 158683.84\n",
      "Œ± =   5000.0 | R¬≤ = 0.8072 | Active = 21/80 | Sparsity =  73.8% | L1 = 93342.31\n",
      "\n",
      "‚úÖ OPTIMAL LASSO REGRESSION MODEL:\n",
      "----------------------------------------\n",
      "Best Œ± (regularization): 5000.0\n",
      "Training R¬≤ score: 0.8072\n",
      "Number of features: 80\n",
      "Active features: 21/80\n",
      "Sparsity: 73.8% of features set to zero\n",
      "Bias term (intercept): 181709.90\n",
      "Non-zero coefficients (first 5): [-2784.31349629  2085.63731527 22065.37810362  2353.15126429\n",
      "   911.78349076]\n",
      "Coefficient L1 norm: 93342.31\n",
      "\n",
      "üìä FEATURE SELECTION ANALYSIS:\n",
      "----------------------------------------\n",
      "Features selected by LASSO: 21\n",
      "Features eliminated by LASSO: 59\n",
      "Feature selection ratio: 26.2%\n",
      "\n",
      "üìà REGULARIZATION PATH ANALYSIS:\n",
      "--------------------------------------------------\n",
      "Œ±        | R¬≤      | Active | Sparsity | L1 Norm\n",
      "--------------------------------------------------\n",
      "     0.0 | 0.8675 |   80   |    0.0% | 242187.98\n",
      "     0.1 | 0.8675 |   80   |    0.0% | 241915.78\n",
      "     1.0 | 0.8675 |   79   |    1.2% | 240441.15\n",
      "    10.0 | 0.8675 |   78   |    2.5% | 235433.79\n",
      "   100.0 | 0.8673 |   74   |    7.5% | 219356.68\n",
      "  1000.0 | 0.8578 |   48   |   40.0% | 158683.84\n",
      "  5000.0 | 0.8072 |   21   |   73.8% | 93342.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hemanthmada/vscodeProjects/ml_assignment_1/.venv/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.811e+09, tolerance: 7.227e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# === STEP 4: LASSO REGRESSION USING SCIKIT-LEARN ===\n",
    "# Using sklearn's Lasso for efficiency and built-in cross-validation\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: LASSO REGRESSION - SCIKIT-LEARN IMPLEMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"üìê Using sklearn LASSO Regression:\")\n",
    "print(\"Objective: Minimize ||XŒ∏ - y||¬≤ + Œª||Œ∏||‚ÇÅ\")\n",
    "print(\"Key Feature: Automatic feature selection via L1 regularization\")\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "\n",
    "# LASSO regression hyperparameter tuning\n",
    "alpha_values = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 5000.0]\n",
    "\n",
    "print(f\"\\nüîç HYPERPARAMETER TUNING - TESTING Œ± VALUES:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Use LassoCV for automatic cross-validation\n",
    "lasso_cv = LassoCV(alphas=alpha_values, cv=5, random_state=42, max_iter=10000)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_alpha = lasso_cv.alpha_\n",
    "print(f\"‚úÖ Best Œ± (regularization) found via CV: {best_alpha}\")\n",
    "\n",
    "# Train final model with best alpha\n",
    "model = Lasso(alpha=best_alpha, max_iter=2000, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test different alpha values for comparison\n",
    "all_results = {}\n",
    "for alpha in alpha_values:\n",
    "    lasso_temp = Lasso(alpha=alpha, max_iter=2000, random_state=42)\n",
    "    lasso_temp.fit(X_train_scaled, y_train)\n",
    "    r2_score = lasso_temp.score(X_train_scaled, y_train)\n",
    "    \n",
    "    # Calculate sparsity metrics\n",
    "    zero_weights = np.sum(np.abs(lasso_temp.coef_) < 1e-6)\n",
    "    active_features = len(lasso_temp.coef_) - zero_weights\n",
    "    sparsity_percent = (zero_weights / len(lasso_temp.coef_)) * 100\n",
    "    \n",
    "    # L1 regularization penalty\n",
    "    reg_penalty = alpha * np.sum(np.abs(lasso_temp.coef_))\n",
    "    \n",
    "    all_results[alpha] = {\n",
    "        'model': lasso_temp,\n",
    "        'r2': r2_score,\n",
    "        'reg_penalty': reg_penalty,\n",
    "        'zero_weights': zero_weights,\n",
    "        'active_features': active_features,\n",
    "        'sparsity': sparsity_percent,\n",
    "        'coef_norm': np.sum(np.abs(lasso_temp.coef_))\n",
    "    }\n",
    "    \n",
    "    print(f\"Œ± = {alpha:8.1f} | R¬≤ = {r2_score:.4f} | Active = {active_features:2d}/{len(lasso_temp.coef_)} | Sparsity = {sparsity_percent:5.1f}% | L1 = {np.sum(np.abs(lasso_temp.coef_)):6.2f}\")\n",
    "\n",
    "# Get results for best model\n",
    "best_results = all_results[best_alpha]\n",
    "\n",
    "print(f\"\\n‚úÖ OPTIMAL LASSO REGRESSION MODEL:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Best Œ± (regularization): {best_alpha}\")\n",
    "print(f\"Training R¬≤ score: {model.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Active features: {best_results['active_features']}/{len(model.coef_)}\")\n",
    "print(f\"Sparsity: {best_results['sparsity']:.1f}% of features set to zero\")\n",
    "print(f\"Bias term (intercept): {model.intercept_:.2f}\")\n",
    "print(f\"Non-zero coefficients (first 5): {model.coef_[np.abs(model.coef_) > 1e-6][:5]}\")\n",
    "print(f\"Coefficient L1 norm: {np.sum(np.abs(model.coef_)):.2f}\")\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model.predict(X_train_scaled)\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Feature selection analysis\n",
    "print(f\"\\nüìä FEATURE SELECTION ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "nonzero_indices = np.where(np.abs(model.coef_) > 1e-6)[0]\n",
    "zero_indices = np.where(np.abs(model.coef_) <= 1e-6)[0]\n",
    "\n",
    "print(f\"Features selected by LASSO: {len(nonzero_indices)}\")\n",
    "print(f\"Features eliminated by LASSO: {len(zero_indices)}\")\n",
    "print(f\"Feature selection ratio: {len(nonzero_indices)/len(model.coef_)*100:.1f}%\")\n",
    "\n",
    "# Compare regularization strengths\n",
    "print(f\"\\nüìà REGULARIZATION PATH ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Œ±        | R¬≤      | Active | Sparsity | L1 Norm\")\n",
    "print(\"-\" * 50)\n",
    "for alpha in sorted(all_results.keys()):\n",
    "    res = all_results[alpha]\n",
    "    print(f\"{alpha:8.1f} | {res['r2']:.4f} |   {res['active_features']:2d}   |  {res['sparsity']:5.1f}% | {res['coef_norm']:7.2f}\")\n",
    "\n",
    "# Store coefficients for analysis (including intercept)\n",
    "weights = np.concatenate([[model.intercept_], model.coef_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d3432bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: ERROR FUNCTION ANALYSIS\n",
      "============================================================\n",
      "üìä TRAINING SET ERROR ANALYSIS:\n",
      "----------------------------------------\n",
      "MSE         : 1,161,282,608\n",
      "RMSE        : $34,077.60\n",
      "MAE         : $20,767.77\n",
      "R¬≤          : 0.8072\n",
      "Adjusted R¬≤ : 0.7934\n",
      "MAPE        : 12.12%\n",
      "\n",
      "üìà PREDICTION QUALITY ANALYSIS:\n",
      "----------------------------------------\n",
      "Mean residual: $-0.00\n",
      "Std of residuals: $34,077.60\n",
      "Min prediction: $27,439.93\n",
      "Max prediction: $542,309.12\n",
      "Predictions in range [0, max_actual]: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# === STEP 5: ERROR FUNCTION ANALYSIS ===\n",
    "# Following course concepts: Multiple Error Functions for Model Evaluation\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: ERROR FUNCTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_error_functions(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate multiple error functions as taught in course\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    \n",
    "    # 1. Mean Squared Error (MSE) - L2 Loss\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # 2. Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # 3. Mean Absolute Error (MAE) - L1 Loss\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # 4. R-squared (Coefficient of Determination)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)  # Residual sum of squares\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    # 5. Adjusted R-squared\n",
    "    n_features = X_train_scaled.shape[1]\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - n_features - 1)\n",
    "    \n",
    "    # 6. Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'Adjusted R¬≤': adj_r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "# Calculate error functions for training data\n",
    "train_errors = calculate_error_functions(y_train, train_predictions)\n",
    "\n",
    "print(\"üìä TRAINING SET ERROR ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "for metric, value in train_errors.items():\n",
    "    if metric in ['MAPE']:\n",
    "        print(f\"{metric:12}: {value:.2f}%\")\n",
    "    elif metric in ['MSE']:\n",
    "        print(f\"{metric:12}: {value:,.0f}\")\n",
    "    elif metric in ['RMSE', 'MAE']:\n",
    "        print(f\"{metric:12}: ${value:,.2f}\")\n",
    "    else:\n",
    "        print(f\"{metric:12}: {value:.4f}\")\n",
    "\n",
    "# Analysis of prediction quality\n",
    "print(f\"\\nüìà PREDICTION QUALITY ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "residuals = y_train - train_predictions\n",
    "print(f\"Mean residual: ${np.mean(residuals):,.2f}\")\n",
    "print(f\"Std of residuals: ${np.std(residuals):,.2f}\")\n",
    "print(f\"Min prediction: ${np.min(train_predictions):,.2f}\")\n",
    "print(f\"Max prediction: ${np.max(train_predictions):,.2f}\")\n",
    "print(f\"Predictions in range [0, max_actual]: {np.sum((train_predictions >= 0) & (train_predictions <= np.max(y_train))) / len(train_predictions) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52e07b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 6: MODEL DIAGNOSTICS AND VALIDATION\n",
      "============================================================\n",
      "üîç PREDICTION EXAMPLES (First 10 samples):\n",
      "--------------------------------------------------\n",
      "     Actual  Predicted   Residual  Abs_Error\n",
      "0  395000.0  269935.31  125064.69  125064.69\n",
      "1  165000.0  179562.73  -14562.73   14562.73\n",
      "2  128200.0  116618.86   11581.14   11581.14\n",
      "3  275000.0  231008.74   43991.26   43991.26\n",
      "4  311872.0  291838.00   20034.00   20034.00\n",
      "5  214000.0  217473.11   -3473.11    3473.11\n",
      "6  153500.0  187667.42  -34167.42   34167.42\n",
      "7  144000.0  163986.41  -19986.41   19986.41\n",
      "8  115000.0  125787.62  -10787.62   10787.62\n",
      "9  180000.0  183519.48   -3519.48    3519.48\n",
      "\n",
      "üìä MODEL COMPLEXITY ANALYSIS:\n",
      "----------------------------------------\n",
      "Number of training samples: 1200\n",
      "Number of features: 80\n",
      "Parameters to data ratio: 81/1200 = 0.0675\n",
      "\n",
      "‚öñÔ∏è LEARNED PARAMETERS ANALYSIS:\n",
      "----------------------------------------\n",
      "Intercept (bias): 181709.8958\n",
      "Largest positive weight: 22065.3781\n",
      "Largest negative weight: -6759.5807\n",
      "Weight standard deviation: 3674.7451\n",
      "\n",
      "üìâ RESIDUAL ANALYSIS:\n",
      "----------------------------------------\n",
      "Residual mean (should be ~0): -0.0000\n",
      "Residual std: 34077.60\n",
      "Residual skewness: 0.0189\n",
      "\n",
      "‚úÖ MODEL VALIDATION COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: MODEL DIAGNOSTICS AND VALIDATION ===\n",
    "# Following course concepts: Model Validation and Diagnostics\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: MODEL DIAGNOSTICS AND VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Prediction Examples (Sample Analysis)\n",
    "print(\"üîç PREDICTION EXAMPLES (First 10 samples):\")\n",
    "print(\"-\" * 50)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_train[:10],\n",
    "    'Predicted': train_predictions[:10],\n",
    "    'Residual': y_train[:10] - train_predictions[:10],\n",
    "    'Abs_Error': np.abs(y_train[:10] - train_predictions[:10])\n",
    "})\n",
    "print(comparison_df.round(2))\n",
    "\n",
    "# 2. Model Complexity Analysis\n",
    "print(f\"\\nüìä MODEL COMPLEXITY ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of training samples: {len(y_train)}\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Parameters to data ratio: {len(weights)}/{len(y_train)} = {len(weights)/len(y_train):.4f}\")\n",
    "\n",
    "# 3. Weight Analysis\n",
    "print(f\"\\n‚öñÔ∏è LEARNED PARAMETERS ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Intercept (bias): {weights[0]:.4f}\")\n",
    "print(f\"Largest positive weight: {np.max(weights[1:]):.4f}\")\n",
    "print(f\"Largest negative weight: {np.min(weights[1:]):.4f}\")\n",
    "print(f\"Weight standard deviation: {np.std(weights[1:]):.4f}\")\n",
    "\n",
    "# 4. Residual Analysis\n",
    "print(f\"\\nüìâ RESIDUAL ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "residuals = y_train - train_predictions\n",
    "print(f\"Residual mean (should be ~0): {np.mean(residuals):.4f}\")\n",
    "print(f\"Residual std: {np.std(residuals):.2f}\")\n",
    "print(f\"Residual skewness: {np.mean(((residuals - np.mean(residuals)) / np.std(residuals)) ** 3):.4f}\")\n",
    "\n",
    "# 5. Prediction Bounds Analysis\n",
    "negative_predictions = np.sum(train_predictions < 0)\n",
    "if negative_predictions > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {negative_predictions} negative predictions detected!\")\n",
    "    print(\"This suggests the model may need constraints or regularization.\")\n",
    "\n",
    "print(f\"\\n‚úÖ MODEL VALIDATION COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7c11c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 7: LASSO MODEL ANALYSIS\n",
      "============================================================\n",
      "üìê LASSO REGRESSION MODEL PROPERTIES:\n",
      "--------------------------------------------------\n",
      "Optimal Œ± (regularization): 5000.0\n",
      "Intercept: 181709.8958\n",
      "Number of coefficients: 80\n",
      "Coefficient L1 norm: 93342.3083\n",
      "Number of zero coefficients: 59\n",
      "Number of active features: 21\n",
      "Sparsity percentage: 73.8%\n",
      "\n",
      "üìä MODEL PERFORMANCE:\n",
      "--------------------------------------------------\n",
      "Training R¬≤ score: 0.8072\n",
      "Training R¬≤ percentage: 80.72%\n",
      "Cross-validation R¬≤ mean: 0.7754 ¬± 0.0522\n",
      "\n",
      "üéØ FEATURE SELECTION ANALYSIS:\n",
      "--------------------------------------------------\n",
      "TOP 10 SELECTED FEATURES:\n",
      "            Feature  Coefficient  Abs_Coefficient\n",
      "     OverallQuality 22065.378104     22065.378104\n",
      "         UsableArea 19904.963637     19904.963637\n",
      "    ParkingCapacity  7273.306611      7273.306611\n",
      "     BasementHeight -6759.580707      6759.580707\n",
      "     KitchenQuality -6355.054174      6355.054174\n",
      "    ExteriorQuality -4848.105726      4848.105726\n",
      "BasementFacilitySF1  4119.289042      4119.289042\n",
      "      PropertyClass -2784.313496      2784.313496\n",
      "            Lounges  2606.745237      2606.745237\n",
      "    GroundFloorArea  2396.047286      2396.047286\n",
      "\n",
      "FEATURES ELIMINATED BY LASSO: 59\n",
      "First 10 eliminated: ['Id', 'ZoningCategory', 'RoadAccessLength', 'RoadType', 'ServiceLaneType', 'PlotShape', 'LandElevation', 'UtilityAccess', 'PlotConfiguration', 'LandSlope']\n",
      "\n",
      "üìà REGULARIZATION PATH ANALYSIS:\n",
      "-------------------------------------------------------\n",
      "Œ±        | R¬≤      | Active | Sparsity | L1 Norm\n",
      "-------------------------------------------------------\n",
      "     0.0 |  0.8675 |     80 |     0.0% | 242187.98\n",
      "     0.1 |  0.8675 |     80 |     0.0% | 241915.78\n",
      "     1.0 |  0.8675 |     79 |     1.2% | 240441.15\n",
      "    10.0 |  0.8675 |     78 |     2.5% | 235433.79\n",
      "   100.0 |  0.8673 |     74 |     7.5% | 219356.68\n",
      "  1000.0 |  0.8578 |     48 |    40.0% | 158683.84\n",
      "  5000.0 |  0.8072 |     21 |    73.8% | 93342.31\n",
      "\n",
      "üî¢ MODEL COMPLEXITY:\n",
      "--------------------------------------------------\n",
      "Number of samples: 1200\n",
      "Total parameters: 81\n",
      "Active parameters: 22\n",
      "Effective degrees of freedom: 1178\n",
      "Parameter/Sample ratio: 0.068\n",
      "Active parameter/Sample ratio: 0.018\n",
      "\n",
      "‚öñÔ∏è SPARSITY BENEFITS:\n",
      "--------------------------------------------------\n",
      "Sparsity ratio: 73.8%\n",
      "Model interpretability: High\n",
      "Regularization penalty: 466,711,541.53\n",
      "Data fit term (MSE): 1,161,282,608\n",
      "Total objective value: 1,627,994,149\n",
      "üîç Strong regularization - aggressive feature selection\n",
      "\n",
      "üèÜ FEATURE SELECTION QUALITY:\n",
      "--------------------------------------------------\n",
      "Average active coefficient magnitude: 4444.8718\n",
      "Maximum coefficient magnitude: 22065.3781\n",
      "Minimum active coefficient magnitude: 360.683867\n",
      "Coefficient range (active): 61.2x\n",
      "‚úÖ LASSO successfully performed automatic feature selection\n"
     ]
    }
   ],
   "source": [
    "# === STEP 7: MODEL ANALYSIS ===\n",
    "# Analysis of the sklearn LASSO Regression model\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: LASSO MODEL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. LASSO Regression Model Properties\n",
    "print(\"üìê LASSO REGRESSION MODEL PROPERTIES:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Optimal Œ± (regularization): {model.alpha}\")\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "print(f\"Number of coefficients: {len(model.coef_)}\")\n",
    "print(f\"Coefficient L1 norm: {np.sum(np.abs(model.coef_)):.4f}\")\n",
    "print(f\"Number of zero coefficients: {np.sum(np.abs(model.coef_) < 1e-6)}\")\n",
    "print(f\"Number of active features: {np.sum(np.abs(model.coef_) >= 1e-6)}\")\n",
    "print(f\"Sparsity percentage: {np.sum(np.abs(model.coef_) < 1e-6)/len(model.coef_)*100:.1f}%\")\n",
    "\n",
    "# 2. Model Performance Metrics\n",
    "print(f\"\\nüìä MODEL PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "train_r2 = model.score(X_train_scaled, y_train)\n",
    "print(f\"Training R¬≤ score: {train_r2:.4f}\")\n",
    "print(f\"Training R¬≤ percentage: {train_r2*100:.2f}%\")\n",
    "\n",
    "# Cross-validation score\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "print(f\"Cross-validation R¬≤ mean: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 3. Feature Selection Analysis\n",
    "print(f\"\\nüéØ FEATURE SELECTION ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "active_mask = np.abs(model.coef_) >= 1e-6\n",
    "zero_mask = np.abs(model.coef_) < 1e-6\n",
    "\n",
    "if hasattr(X_train_processed, 'columns'):\n",
    "    feature_names = X_train_processed.columns\n",
    "    \n",
    "    # Active features\n",
    "    active_features_df = pd.DataFrame({\n",
    "        'Feature': feature_names[active_mask],\n",
    "        'Coefficient': model.coef_[active_mask],\n",
    "        'Abs_Coefficient': np.abs(model.coef_[active_mask])\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"TOP 10 SELECTED FEATURES:\")\n",
    "    print(active_features_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Zero features (eliminated)\n",
    "    zero_features = feature_names[zero_mask]\n",
    "    print(f\"\\nFEATURES ELIMINATED BY LASSO: {len(zero_features)}\")\n",
    "    if len(zero_features) <= 10:\n",
    "        print(f\"Eliminated features: {list(zero_features)}\")\n",
    "    else:\n",
    "        print(f\"First 10 eliminated: {list(zero_features[:10])}\")\n",
    "else:\n",
    "    # If we don't have feature names, show indices\n",
    "    active_indices = np.where(active_mask)[0]\n",
    "    zero_indices = np.where(zero_mask)[0]\n",
    "    \n",
    "    print(\"TOP 10 SELECTED FEATURE INDICES:\")\n",
    "    coef_indices = np.argsort(np.abs(model.coef_[active_mask]))[::-1][:10]\n",
    "    for i, idx in enumerate(coef_indices):\n",
    "        feature_idx = active_indices[idx]\n",
    "        print(f\"Feature {feature_idx:2d}: {model.coef_[feature_idx]:8.4f}\")\n",
    "    \n",
    "    print(f\"\\nFEATURES ELIMINATED: {len(zero_indices)} features\")\n",
    "\n",
    "# 4. Regularization Path Analysis\n",
    "print(f\"\\nüìà REGULARIZATION PATH ANALYSIS:\")\n",
    "print(\"-\" * 55)\n",
    "print(\"Œ±        | R¬≤      | Active | Sparsity | L1 Norm\")\n",
    "print(\"-\" * 55)\n",
    "for alpha in sorted(all_results.keys()):\n",
    "    result = all_results[alpha]\n",
    "    print(f\"{alpha:8.1f} | {result['r2']:7.4f} | {result['active_features']:6d} | {result['sparsity']:7.1f}% | {result['coef_norm']:7.2f}\")\n",
    "\n",
    "# 5. Model Complexity Analysis\n",
    "print(f\"\\nüî¢ MODEL COMPLEXITY:\")\n",
    "print(\"-\" * 50)\n",
    "n_samples = len(y_train)\n",
    "n_params = len(model.coef_) + 1  # +1 for intercept\n",
    "active_params = np.sum(np.abs(model.coef_) >= 1e-6) + 1  # +1 for intercept\n",
    "df_residual = n_samples - active_params\n",
    "\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Total parameters: {n_params}\")\n",
    "print(f\"Active parameters: {active_params}\")\n",
    "print(f\"Effective degrees of freedom: {df_residual}\")\n",
    "print(f\"Parameter/Sample ratio: {n_params/n_samples:.3f}\")\n",
    "print(f\"Active parameter/Sample ratio: {active_params/n_samples:.3f}\")\n",
    "\n",
    "# 6. Sparsity Benefits Assessment\n",
    "print(f\"\\n‚öñÔ∏è SPARSITY BENEFITS:\")\n",
    "print(\"-\" * 50)\n",
    "sparsity_ratio = np.sum(np.abs(model.coef_) < 1e-6) / len(model.coef_)\n",
    "reg_penalty = model.alpha * np.sum(np.abs(model.coef_))\n",
    "data_fit = np.mean((y_train - train_predictions) ** 2)\n",
    "\n",
    "print(f\"Sparsity ratio: {sparsity_ratio*100:.1f}%\")\n",
    "print(f\"Model interpretability: {'High' if sparsity_ratio > 0.5 else 'Medium' if sparsity_ratio > 0.2 else 'Low'}\")\n",
    "print(f\"Regularization penalty: {reg_penalty:,.2f}\")\n",
    "print(f\"Data fit term (MSE): {data_fit:,.0f}\")\n",
    "print(f\"Total objective value: {data_fit + reg_penalty:,.0f}\")\n",
    "\n",
    "# Assessment of regularization strength\n",
    "if model.alpha < 1.0:\n",
    "    print(\"‚úÖ Light regularization - minimal feature selection\")\n",
    "elif model.alpha < 100.0:\n",
    "    print(\"‚öñÔ∏è Moderate regularization - balanced feature selection\")\n",
    "else:\n",
    "    print(\"üîç Strong regularization - aggressive feature selection\")\n",
    "\n",
    "# 7. Feature Selection Quality\n",
    "print(f\"\\nüèÜ FEATURE SELECTION QUALITY:\")\n",
    "print(\"-\" * 50)\n",
    "if np.sum(active_mask) > 0:\n",
    "    avg_coef_magnitude = np.mean(np.abs(model.coef_[active_mask]))\n",
    "    max_coef_magnitude = np.max(np.abs(model.coef_))\n",
    "    min_active_coef = np.min(np.abs(model.coef_[active_mask]))\n",
    "    \n",
    "    print(f\"Average active coefficient magnitude: {avg_coef_magnitude:.4f}\")\n",
    "    print(f\"Maximum coefficient magnitude: {max_coef_magnitude:.4f}\")\n",
    "    print(f\"Minimum active coefficient magnitude: {min_active_coef:.6f}\")\n",
    "    print(f\"Coefficient range (active): {max_coef_magnitude/min_active_coef:.1f}x\")\n",
    "    \n",
    "    print(\"‚úÖ LASSO successfully performed automatic feature selection\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è All features eliminated - consider reducing regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efef159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 8: KAGGLE SUBMISSION PREPARATION\n",
      "============================================================\n",
      "üìÅ LOADING TEST DATA AND MAKING PREDICTIONS:\n",
      "--------------------------------------------------\n",
      "Test data shape: (260, 80)\n",
      "‚úÖ Test predictions generated for 260 samples\n",
      "Prediction range: [41185.43, 455742.31]\n",
      "üì§ SUBMISSION FILE CREATED:\n",
      "--------------------------------------------------\n",
      "File saved at: /Users/hemanthmada/vscodeProjects/ml_assignment_1/submissions/3_LASSO_regression_sklearn.csv\n",
      "Submission shape: (260, 2)\n",
      "\n",
      "First 5 predictions:\n",
      "     Id     HotelValue\n",
      "0   893  139107.786445\n",
      "1  1106  303128.881744\n",
      "2   414  121850.617881\n",
      "3   523  169622.270272\n",
      "4  1037  308451.176528\n",
      "\n",
      "============================================================\n",
      "üéì SKLEARN LASSO REGRESSION SUMMARY\n",
      "============================================================\n",
      "‚úÖ Step 1: Data Loading and Exploration - COMPLETED\n",
      "‚úÖ Step 2: Data Preprocessing and Feature Engineering - COMPLETED\n",
      "‚úÖ Step 3: Feature Scaling - COMPLETED\n",
      "‚úÖ Step 4: Sklearn LASSO Regression with CV - COMPLETED\n",
      "‚úÖ Step 5: Comprehensive Error Function Analysis - COMPLETED\n",
      "‚úÖ Step 6: Model Diagnostics and Validation - COMPLETED\n",
      "‚úÖ Step 7: LASSO Model Analysis - COMPLETED\n",
      "‚úÖ Step 8: Kaggle Submission Preparation - COMPLETED\n",
      "\n",
      "üèÜ SKLEARN LASSO REGRESSION MODEL READY!\n",
      "üéØ Optimal Œ±: 5000.0 | Training R¬≤: 0.8072\n",
      "üìä Cross-validation R¬≤: 0.7754 ¬± 0.0522\n",
      "üîç Feature Selection: 21/80 active features (73.8% sparsity)\n"
     ]
    }
   ],
   "source": [
    "# === STEP 8: KAGGLE SUBMISSION PREPARATION ===\n",
    "# Final step: Prepare submission file for Kaggle competition\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8: KAGGLE SUBMISSION PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load test data and make predictions\n",
    "print(\"üìÅ LOADING TEST DATA AND MAKING PREDICTIONS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test data was already preprocessed and scaled in Step 3\n",
    "print(f\"Test data shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# Make predictions using our trained sklearn LASSO model\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"‚úÖ Test predictions generated for {len(test_predictions)} samples\")\n",
    "print(f\"Prediction range: [{test_predictions.min():.2f}, {test_predictions.max():.2f}]\")\n",
    "\n",
    "# Create submission file using actual test IDs\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_df['Id'].values,  # Use actual IDs from test dataset\n",
    "    'HotelValue': test_predictions  # Use HotelValue as per sample submission format\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = '/Users/hemanthmada/vscodeProjects/ml_assignment_1/submissions/3_LASSO_regression_sklearn.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"üì§ SUBMISSION FILE CREATED:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"File saved at: {submission_path}\")\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Final summary of the entire LASSO regression implementation\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üéì SKLEARN LASSO REGRESSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Step 1: Data Loading and Exploration - COMPLETED\")\n",
    "print(\"‚úÖ Step 2: Data Preprocessing and Feature Engineering - COMPLETED\")\n",
    "print(\"‚úÖ Step 3: Feature Scaling - COMPLETED\")\n",
    "print(\"‚úÖ Step 4: Sklearn LASSO Regression with CV - COMPLETED\")\n",
    "print(\"‚úÖ Step 5: Comprehensive Error Function Analysis - COMPLETED\")\n",
    "print(\"‚úÖ Step 6: Model Diagnostics and Validation - COMPLETED\")\n",
    "print(\"‚úÖ Step 7: LASSO Model Analysis - COMPLETED\")\n",
    "print(\"‚úÖ Step 8: Kaggle Submission Preparation - COMPLETED\")\n",
    "print(f\"\\nüèÜ SKLEARN LASSO REGRESSION MODEL READY!\")\n",
    "print(f\"üéØ Optimal Œ±: {model.alpha} | Training R¬≤: {model.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"üìä Cross-validation R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "print(f\"üîç Feature Selection: {np.sum(np.abs(model.coef_) >= 1e-6)}/{len(model.coef_)} active features ({np.sum(np.abs(model.coef_) < 1e-6)/len(model.coef_)*100:.1f}% sparsity)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
