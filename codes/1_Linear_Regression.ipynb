{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd99b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e824b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MACHINE LEARNING ASSIGNMENT - LINEAR REGRESSION\n",
      "============================================================\n",
      "‚úÖ Files loaded successfully!\n",
      "Training data shape: (1200, 81)\n",
      "Test data shape: (260, 80)\n",
      "\n",
      "üìä Target variable: HotelValue\n",
      "Target statistics:\n",
      "count      1200.000000\n",
      "mean     181709.895833\n",
      "std       77638.660223\n",
      "min       34900.000000\n",
      "25%      130000.000000\n",
      "50%      165000.000000\n",
      "75%      215000.000000\n",
      "max      745000.000000\n",
      "Name: HotelValue, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Load the datasets ---\n",
    "print(\"=\"*60)\n",
    "print(\"MACHINE LEARNING ASSIGNMENT - LINEAR REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Reading from the Hotel-Property-Value-Dataset folder\n",
    "    train_df = pd.read_csv('../Hotel-Property-Value-Dataset/train.csv')\n",
    "    test_df = pd.read_csv('../Hotel-Property-Value-Dataset/test.csv')\n",
    "    sample_submission_df = pd.read_csv('../Hotel-Property-Value-Dataset/sample_submission.csv')\n",
    "    CACHE_DIR = '../Hotel-Property-Value-Dataset/'  # Change this path as needed\n",
    "    print(\"‚úÖ Files loaded successfully!\")\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Ensure the Hotel-Property-Value-Dataset folder contains train.csv, test.csv, and sample_submission.csv\")\n",
    "    exit()\n",
    "\n",
    "# --- Target Variable ---\n",
    "TARGET_VARIABLE = \"HotelValue\"\n",
    "\n",
    "if TARGET_VARIABLE not in train_df.columns:\n",
    "    print(f\"‚ùå Error: The target column '{TARGET_VARIABLE}' was not found in train.csv.\")\n",
    "    print(f\"Available columns are: {list(train_df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nüìä Target variable: {TARGET_VARIABLE}\")\n",
    "print(f\"Target statistics:\\n{train_df[TARGET_VARIABLE].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15c3947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: DATA PREPROCESSING\n",
      "============================================================\n",
      "Features in training data: 80\n",
      "Features in test data: 80\n",
      "\n",
      "üìà Numeric features (37): ['Id', 'PropertyClass', 'RoadAccessLength', 'LandArea', 'OverallQuality']...\n",
      "üìù Categorical features (43): ['ZoningCategory', 'RoadType', 'ServiceLaneType', 'PlotShape', 'LandElevation']...\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Data Preprocessing (Course Concepts) ---\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Separate features (X) from the target (y)\n",
    "X_train_full = train_df.drop([TARGET_VARIABLE], axis=1)\n",
    "y_train = train_df[TARGET_VARIABLE]\n",
    "X_test_full = test_df.copy()\n",
    "\n",
    "print(f\"Features in training data: {X_train_full.shape[1]}\")\n",
    "print(f\"Features in test data: {X_test_full.shape[1]}\")\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train_full.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nüìà Numeric features ({len(numeric_features)}): {numeric_features[:5]}...\" if len(numeric_features) > 5 else f\"\\nüìà Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"üìù Categorical features ({len(categorical_features)}): {categorical_features[:5]}...\" if len(categorical_features) > 5 else f\"üìù Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ace009ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned data loaded from ../Hotel-Property-Value-Dataset/train_cleaned_mada.csv\n",
      "   Shape: (1200, 81)\n",
      "‚úÖ Using cleaned data from CSV!\n",
      "Training features shape: (1200, 80)\n",
      "Target shape: (1200,)\n",
      "üîÑ Processing test data...\n",
      "\n",
      "============================================================\n",
      "STEP 3: FEATURE SCALING\n",
      "============================================================\n",
      "‚úÖ Features scaled successfully!\n",
      "Training features shape: (1200, 80)\n",
      "Test features shape: (260, 80)\n",
      "Feature means after scaling: [-9.47390314e-17  1.77635684e-17  2.54611147e-16 -7.69754630e-17\n",
      " -6.21724894e-17] (should be ~0)\n",
      "Feature stds after scaling: [1. 1. 1. 1. 1.] (should be ~1)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Handle Missing Values and Encode Categorical Variables ---\n",
    "\n",
    "# Configuration: Set your preferred cache directory here\n",
    "CACHE_DIR = '../Hotel-Property-Value-Dataset/'  # Change this path as needed\n",
    "\n",
    "def save_cleaned_data(X_train_processed, y_train, base_path=CACHE_DIR):\n",
    "    \"\"\"Save cleaned data as CSV file\"\"\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # Combine features and target into single dataframe\n",
    "    cleaned_df = X_train_processed.copy()\n",
    "    cleaned_df['HotelValue'] = y_train\n",
    "    \n",
    "    # Save as CSV\n",
    "    csv_path = os.path.join(base_path, 'train_cleaned_mada.csv')\n",
    "    cleaned_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Cleaned data saved to {csv_path}\")\n",
    "    print(f\"   Shape: {cleaned_df.shape}\")\n",
    "    return csv_path\n",
    "\n",
    "def load_cleaned_data(base_path=CACHE_DIR):\n",
    "    \"\"\"Load cleaned data if it exists\"\"\"\n",
    "    csv_path = os.path.join(base_path, 'train_cleaned_mada.csv')\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        cleaned_df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X_train_processed = cleaned_df.drop('HotelValue', axis=1)\n",
    "        y_train = cleaned_df['HotelValue']\n",
    "        \n",
    "        print(f\"‚úÖ Cleaned data loaded from {csv_path}\")\n",
    "        print(f\"   Shape: {cleaned_df.shape}\")\n",
    "        return X_train_processed, y_train, True\n",
    "    else:\n",
    "        print(\"‚ùå Cleaned data not found. Will perform preprocessing...\")\n",
    "        return None, None, False\n",
    "\n",
    "def preprocess_data(X_train, X_test, numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Preprocess the data by handling missing values and encoding categorical variables\n",
    "    Following course preprocessing concepts\n",
    "    \"\"\"\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_test_processed = X_test.copy()\n",
    "    \n",
    "    # Handle numeric features - Fill with median (robust to outliers)\n",
    "    for col in numeric_features:\n",
    "        if col in X_train_processed.columns:\n",
    "            median_val = X_train_processed[col].median()\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(median_val)\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(median_val)\n",
    "    \n",
    "    # Handle categorical features - Label encoding\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        if col in X_train_processed.columns:\n",
    "            # Fill missing values with mode (most frequent value)\n",
    "            mode_val = X_train_processed[col].mode()[0] if not X_train_processed[col].mode().empty else 'Unknown'\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(mode_val)\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(mode_val)\n",
    "            \n",
    "            # Label encode categorical variables\n",
    "            le = LabelEncoder()\n",
    "            # Fit on combined data to handle unseen categories in test set\n",
    "            combined_data = pd.concat([X_train_processed[col], X_test_processed[col]], axis=0)\n",
    "            le.fit(combined_data)\n",
    "            \n",
    "            X_train_processed[col] = le.transform(X_train_processed[col])\n",
    "            X_test_processed[col] = le.transform(X_test_processed[col])\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return X_train_processed, X_test_processed, label_encoders\n",
    "\n",
    "# Try to load cleaned data first\n",
    "X_train_processed, y_train_series, data_loaded = load_cleaned_data()\n",
    "\n",
    "if not data_loaded:\n",
    "    print(\"üîÑ Starting data preprocessing...\")\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_processed, X_test_processed, label_encoders = preprocess_data(\n",
    "        X_train_full, X_test_full, numeric_features, categorical_features\n",
    "    )\n",
    "\n",
    "    print(f\"\\nAfter preprocessing:\")\n",
    "    print(f\"‚úÖ Training data shape: {X_train_processed.shape}\")\n",
    "    print(f\"‚úÖ Test data shape: {X_test_processed.shape}\")\n",
    "    print(f\"‚úÖ Missing values in training data: {X_train_processed.isnull().sum().sum()}\")\n",
    "    print(f\"‚úÖ Missing values in test data: {X_test_processed.isnull().sum().sum()}\")\n",
    "\n",
    "    # Save cleaned training data as CSV\n",
    "    y_train_for_saving = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "    save_cleaned_data(X_train_processed, y_train_for_saving)\n",
    "    \n",
    "    # Convert y_train to pandas Series for consistency\n",
    "    y_train_series = pd.Series(y_train_for_saving, name='HotelValue')\n",
    "    \n",
    "else:\n",
    "    print(f\"‚úÖ Using cleaned data from CSV!\")\n",
    "    print(f\"Training features shape: {X_train_processed.shape}\")\n",
    "    print(f\"Target shape: {y_train_series.shape}\")\n",
    "    \n",
    "    # Still need to preprocess test data when loading from CSV\n",
    "    print(\"üîÑ Processing test data...\")\n",
    "    _, X_test_processed, label_encoders = preprocess_data(\n",
    "        X_train_full, X_test_full, numeric_features, categorical_features\n",
    "    )\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: FEATURE SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale all features using standardization (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_processed)\n",
    "X_test_scaled = scaler.transform(X_test_processed)\n",
    "\n",
    "print(f\"‚úÖ Features scaled successfully!\")\n",
    "print(f\"Training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test features shape: {X_test_scaled.shape}\")\n",
    "print(f\"Feature means after scaling: {np.mean(X_train_scaled, axis=0)[:5]} (should be ~0)\")\n",
    "print(f\"Feature stds after scaling: {np.std(X_train_scaled, axis=0)[:5]} (should be ~1)\")\n",
    "\n",
    "# Convert to numpy array for mathematical operations\n",
    "y_train = y_train_series.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99668638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: LINEAR REGRESSION - SCIKIT-LEARN IMPLEMENTATION\n",
      "============================================================\n",
      "üìê Using sklearn LinearRegression:\n",
      "Automatically handles bias term and uses optimized solvers\n",
      "Training data shape: (1200, 80)\n",
      "‚úÖ Model training completed successfully!\n",
      "Number of features: 80\n",
      "Bias term (intercept): 181709.90\n",
      "First 5 feature weights: [ -391.28508795 -4022.24917556 -1685.55390204 -4082.45730985\n",
      "  4347.9637522 ]\n",
      "Model R¬≤ score on training data: 0.8675\n"
     ]
    }
   ],
   "source": [
    "# === STEP 4: LINEAR REGRESSION USING SCIKIT-LEARN ===\n",
    "# Using sklearn's LinearRegression for simplicity and efficiency\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: LINEAR REGRESSION - SCIKIT-LEARN IMPLEMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"üìê Using sklearn LinearRegression:\")\n",
    "print(\"Automatically handles bias term and uses optimized solvers\")\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Model training completed successfully!\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Bias term (intercept): {model.intercept_:.2f}\")\n",
    "print(f\"First 5 feature weights: {model.coef_[:5]}\")\n",
    "print(f\"Model R¬≤ score on training data: {model.score(X_train_scaled, y_train):.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model.predict(X_train_scaled)\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Store coefficients for analysis (including intercept)\n",
    "weights = np.concatenate([[model.intercept_], model.coef_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d3432bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: ERROR FUNCTION ANALYSIS\n",
      "============================================================\n",
      "üìä TRAINING SET ERROR ANALYSIS:\n",
      "----------------------------------------\n",
      "MSE         : 797,912,991\n",
      "RMSE        : $28,247.35\n",
      "MAE         : $17,996.59\n",
      "R¬≤          : 0.8675\n",
      "Adjusted R¬≤ : 0.8580\n",
      "MAPE        : 10.44%\n",
      "\n",
      "üìà PREDICTION QUALITY ANALYSIS:\n",
      "----------------------------------------\n",
      "Mean residual: $-0.00\n",
      "Std of residuals: $28,247.35\n",
      "Min prediction: $28,764.84\n",
      "Max prediction: $655,640.59\n",
      "Predictions in range [0, max_actual]: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# === STEP 5: ERROR FUNCTION ANALYSIS ===\n",
    "# Following course concepts: Multiple Error Functions for Model Evaluation\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: ERROR FUNCTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_error_functions(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate multiple error functions as taught in course\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    \n",
    "    # 1. Mean Squared Error (MSE) - L2 Loss\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # 2. Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # 3. Mean Absolute Error (MAE) - L1 Loss\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # 4. R-squared (Coefficient of Determination)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)  # Residual sum of squares\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    # 5. Adjusted R-squared\n",
    "    n_features = X_train_scaled.shape[1]\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - n_features - 1)\n",
    "    \n",
    "    # 6. Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'Adjusted R¬≤': adj_r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "# Calculate error functions for training data\n",
    "train_errors = calculate_error_functions(y_train, train_predictions)\n",
    "\n",
    "print(\"üìä TRAINING SET ERROR ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "for metric, value in train_errors.items():\n",
    "    if metric in ['MAPE']:\n",
    "        print(f\"{metric:12}: {value:.2f}%\")\n",
    "    elif metric in ['MSE']:\n",
    "        print(f\"{metric:12}: {value:,.0f}\")\n",
    "    elif metric in ['RMSE', 'MAE']:\n",
    "        print(f\"{metric:12}: ${value:,.2f}\")\n",
    "    else:\n",
    "        print(f\"{metric:12}: {value:.4f}\")\n",
    "\n",
    "# Analysis of prediction quality\n",
    "print(f\"\\nüìà PREDICTION QUALITY ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "residuals = y_train - train_predictions\n",
    "print(f\"Mean residual: ${np.mean(residuals):,.2f}\")\n",
    "print(f\"Std of residuals: ${np.std(residuals):,.2f}\")\n",
    "print(f\"Min prediction: ${np.min(train_predictions):,.2f}\")\n",
    "print(f\"Max prediction: ${np.max(train_predictions):,.2f}\")\n",
    "print(f\"Predictions in range [0, max_actual]: {np.sum((train_predictions >= 0) & (train_predictions <= np.max(y_train))) / len(train_predictions) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52e07b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 6: MODEL DIAGNOSTICS AND VALIDATION\n",
      "============================================================\n",
      "üîç PREDICTION EXAMPLES (First 10 samples):\n",
      "--------------------------------------------------\n",
      "     Actual  Predicted   Residual  Abs_Error\n",
      "0  395000.0  290355.17  104644.83  104644.83\n",
      "1  165000.0  191453.87  -26453.87   26453.87\n",
      "2  128200.0  122185.24    6014.76    6014.76\n",
      "3  275000.0  250479.69   24520.31   24520.31\n",
      "4  311872.0  342726.02  -30854.02   30854.02\n",
      "5  214000.0  233483.13  -19483.13   19483.13\n",
      "6  153500.0  183654.24  -30154.24   30154.24\n",
      "7  144000.0  148961.69   -4961.69    4961.69\n",
      "8  115000.0  118072.04   -3072.04    3072.04\n",
      "9  180000.0  170391.32    9608.68    9608.68\n",
      "\n",
      "üìä MODEL COMPLEXITY ANALYSIS:\n",
      "----------------------------------------\n",
      "Number of training samples: 1200\n",
      "Number of features: 80\n",
      "Parameters to data ratio: 81/1200 = 0.0675\n",
      "\n",
      "‚öñÔ∏è LEARNED PARAMETERS ANALYSIS:\n",
      "----------------------------------------\n",
      "Intercept (bias): 181709.8958\n",
      "Largest positive weight: 16213.7466\n",
      "Largest negative weight: -21155.7561\n",
      "Weight standard deviation: 4715.9311\n",
      "\n",
      "üìâ RESIDUAL ANALYSIS:\n",
      "----------------------------------------\n",
      "Residual mean (should be ~0): -0.0000\n",
      "Residual std: 28247.35\n",
      "Residual skewness: -0.4515\n",
      "\n",
      "‚úÖ MODEL VALIDATION COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: MODEL DIAGNOSTICS AND VALIDATION ===\n",
    "# Following course concepts: Model Validation and Diagnostics\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: MODEL DIAGNOSTICS AND VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Prediction Examples (Sample Analysis)\n",
    "print(\"üîç PREDICTION EXAMPLES (First 10 samples):\")\n",
    "print(\"-\" * 50)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_train[:10],\n",
    "    'Predicted': train_predictions[:10],\n",
    "    'Residual': y_train[:10] - train_predictions[:10],\n",
    "    'Abs_Error': np.abs(y_train[:10] - train_predictions[:10])\n",
    "})\n",
    "print(comparison_df.round(2))\n",
    "\n",
    "# 2. Model Complexity Analysis\n",
    "print(f\"\\nüìä MODEL COMPLEXITY ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of training samples: {len(y_train)}\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Parameters to data ratio: {len(weights)}/{len(y_train)} = {len(weights)/len(y_train):.4f}\")\n",
    "\n",
    "# 3. Weight Analysis\n",
    "print(f\"\\n‚öñÔ∏è LEARNED PARAMETERS ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Intercept (bias): {weights[0]:.4f}\")\n",
    "print(f\"Largest positive weight: {np.max(weights[1:]):.4f}\")\n",
    "print(f\"Largest negative weight: {np.min(weights[1:]):.4f}\")\n",
    "print(f\"Weight standard deviation: {np.std(weights[1:]):.4f}\")\n",
    "\n",
    "# 4. Residual Analysis\n",
    "print(f\"\\nüìâ RESIDUAL ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "residuals = y_train - train_predictions\n",
    "print(f\"Residual mean (should be ~0): {np.mean(residuals):.4f}\")\n",
    "print(f\"Residual std: {np.std(residuals):.2f}\")\n",
    "print(f\"Residual skewness: {np.mean(((residuals - np.mean(residuals)) / np.std(residuals)) ** 3):.4f}\")\n",
    "\n",
    "# 5. Prediction Bounds Analysis\n",
    "negative_predictions = np.sum(train_predictions < 0)\n",
    "if negative_predictions > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {negative_predictions} negative predictions detected!\")\n",
    "    print(\"This suggests the model may need constraints or regularization.\")\n",
    "\n",
    "print(f\"\\n‚úÖ MODEL VALIDATION COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7c11c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 7: MODEL ANALYSIS\n",
      "============================================================\n",
      "üìê MODEL COEFFICIENTS ANALYSIS:\n",
      "--------------------------------------------------\n",
      "Intercept: 181709.8958\n",
      "Number of coefficients: 80\n",
      "Largest positive coefficient: 16213.7466\n",
      "Largest negative coefficient: -21155.7561\n",
      "Coefficient standard deviation: 4715.9311\n",
      "\n",
      "üìä MODEL PERFORMANCE:\n",
      "--------------------------------------------------\n",
      "Training R¬≤ score: 0.8675\n",
      "Training R¬≤ percentage: 86.75%\n",
      "\n",
      "üéØ TOP 10 MOST IMPORTANT FEATURES:\n",
      "--------------------------------------------------\n",
      "         Feature   Coefficient  Abs_Coefficient\n",
      "     PoolQuality -21155.756144     21155.756144\n",
      "SwimmingPoolArea  16213.746554     16213.746554\n",
      "  OverallQuality  14426.041098     14426.041098\n",
      "      UsableArea  12696.454277     12696.454277\n",
      " GroundFloorArea   8996.209182      8996.209182\n",
      "  BasementHeight  -8053.076359      8053.076359\n",
      "  UpperFloorArea   7324.226492      7324.226492\n",
      "      TotalRooms   6932.613908      6932.613908\n",
      " ExteriorQuality  -6872.394345      6872.394345\n",
      "  KitchenQuality  -6562.081482      6562.081482\n",
      "\n",
      "üî¢ MODEL COMPLEXITY:\n",
      "--------------------------------------------------\n",
      "Number of samples: 1200\n",
      "Number of parameters: 81\n",
      "Degrees of freedom (residual): 1119\n",
      "Parameter/Sample ratio: 0.068\n",
      "‚úÖ Appropriate parameter to sample ratio\n"
     ]
    }
   ],
   "source": [
    "# === STEP 7: MODEL ANALYSIS ===\n",
    "# Analysis of the sklearn LinearRegression model\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: MODEL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Model Coefficients Analysis\n",
    "print(\"üìê MODEL COEFFICIENTS ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "print(f\"Number of coefficients: {len(model.coef_)}\")\n",
    "print(f\"Largest positive coefficient: {np.max(model.coef_):.4f}\")\n",
    "print(f\"Largest negative coefficient: {np.min(model.coef_):.4f}\")\n",
    "print(f\"Coefficient standard deviation: {np.std(model.coef_):.4f}\")\n",
    "\n",
    "# 2. Model Performance Metrics\n",
    "print(f\"\\nüìä MODEL PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "train_r2 = model.score(X_train_scaled, y_train)\n",
    "print(f\"Training R¬≤ score: {train_r2:.4f}\")\n",
    "print(f\"Training R¬≤ percentage: {train_r2*100:.2f}%\")\n",
    "\n",
    "# 3. Feature Importance (absolute coefficient values)\n",
    "print(f\"\\nüéØ TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "if hasattr(X_train_processed, 'columns'):\n",
    "    feature_names = X_train_processed.columns\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': model.coef_,\n",
    "        'Abs_Coefficient': np.abs(model.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "else:\n",
    "    # If we don't have feature names, show indices\n",
    "    coef_indices = np.argsort(np.abs(model.coef_))[::-1][:10]\n",
    "    for i, idx in enumerate(coef_indices):\n",
    "        print(f\"Feature {idx:2d}: {model.coef_[idx]:8.4f}\")\n",
    "\n",
    "# 4. Model Complexity Analysis\n",
    "print(f\"\\nüî¢ MODEL COMPLEXITY:\")\n",
    "print(\"-\" * 50)\n",
    "n_samples = len(y_train)\n",
    "n_params = len(model.coef_) + 1  # +1 for intercept\n",
    "df_residual = n_samples - n_params\n",
    "\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Number of parameters: {n_params}\")\n",
    "print(f\"Degrees of freedom (residual): {df_residual}\")\n",
    "print(f\"Parameter/Sample ratio: {n_params/n_samples:.3f}\")\n",
    "\n",
    "if n_params/n_samples > 0.1:\n",
    "    print(\"‚ö†Ô∏è High parameter to sample ratio - potential overfitting risk\")\n",
    "else:\n",
    "    print(\"‚úÖ Appropriate parameter to sample ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efef159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 8: KAGGLE SUBMISSION PREPARATION\n",
      "============================================================\n",
      "üìÅ LOADING TEST DATA AND MAKING PREDICTIONS:\n",
      "--------------------------------------------------\n",
      "Test data shape: (260, 80)\n",
      "‚úÖ Test predictions generated for 260 samples\n",
      "Prediction range: [-93673.17, 526824.15]\n",
      "üì§ SUBMISSION FILE CREATED:\n",
      "--------------------------------------------------\n",
      "File saved at: /Users/hemanthmada/vscodeProjects/ml_assignment_1/submissions/1_linear_regression_sklearn.csv\n",
      "Submission shape: (260, 2)\n",
      "\n",
      "First 5 predictions:\n",
      "     Id     HotelValue\n",
      "0   893  148643.079254\n",
      "1  1106  326768.889709\n",
      "2   414  108922.284974\n",
      "3   523  172606.454613\n",
      "4  1037  316243.612234\n",
      "\n",
      "============================================================\n",
      "üéì SKLEARN LINEAR REGRESSION SUMMARY\n",
      "============================================================\n",
      "‚úÖ Step 1: Data Loading and Exploration - COMPLETED\n",
      "‚úÖ Step 2: Data Preprocessing and Feature Engineering - COMPLETED\n",
      "‚úÖ Step 3: Feature Scaling - COMPLETED\n",
      "‚úÖ Step 4: Sklearn LinearRegression Training - COMPLETED\n",
      "‚úÖ Step 5: Comprehensive Error Function Analysis - COMPLETED\n",
      "‚úÖ Step 6: Model Diagnostics and Validation - COMPLETED\n",
      "‚úÖ Step 7: Model Analysis - COMPLETED\n",
      "‚úÖ Step 8: Kaggle Submission Preparation - COMPLETED\n",
      "\n",
      "üèÜ SKLEARN LINEAR REGRESSION MODEL READY!\n",
      "üìà Final Training R¬≤ Score: 0.8675\n"
     ]
    }
   ],
   "source": [
    "# === STEP 8: KAGGLE SUBMISSION PREPARATION ===\n",
    "# Final step: Prepare submission file for Kaggle competition\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8: KAGGLE SUBMISSION PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load test data and make predictions\n",
    "print(\"üìÅ LOADING TEST DATA AND MAKING PREDICTIONS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test data was already preprocessed and scaled in Step 3\n",
    "print(f\"Test data shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# Make predictions using our trained sklearn model\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"‚úÖ Test predictions generated for {len(test_predictions)} samples\")\n",
    "print(f\"Prediction range: [{test_predictions.min():.2f}, {test_predictions.max():.2f}]\")\n",
    "\n",
    "# Create submission file using actual test IDs\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_df['Id'].values,  # Use actual IDs from test dataset\n",
    "    'HotelValue': test_predictions  # Use HotelValue as per sample submission format\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = '/Users/hemanthmada/vscodeProjects/ml_assignment_1/submissions/1_linear_regression_sklearn.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"üì§ SUBMISSION FILE CREATED:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"File saved at: {submission_path}\")\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Final summary of the entire linear regression implementation\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üéì SKLEARN LINEAR REGRESSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Step 1: Data Loading and Exploration - COMPLETED\")\n",
    "print(\"‚úÖ Step 2: Data Preprocessing and Feature Engineering - COMPLETED\")\n",
    "print(\"‚úÖ Step 3: Feature Scaling - COMPLETED\")\n",
    "print(\"‚úÖ Step 4: Sklearn LinearRegression Training - COMPLETED\")\n",
    "print(\"‚úÖ Step 5: Comprehensive Error Function Analysis - COMPLETED\")\n",
    "print(\"‚úÖ Step 6: Model Diagnostics and Validation - COMPLETED\")\n",
    "print(\"‚úÖ Step 7: Model Analysis - COMPLETED\")\n",
    "print(\"‚úÖ Step 8: Kaggle Submission Preparation - COMPLETED\")\n",
    "print(\"\\nüèÜ SKLEARN LINEAR REGRESSION MODEL READY!\")\n",
    "print(f\"üìà Final Training R¬≤ Score: {model.score(X_train_scaled, y_train):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
